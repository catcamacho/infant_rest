{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infant resting state fMRI preprocessing\n",
    "This notebook contains preprocessing tailored to infant resting state fMRI collected in 5-8 month olds. \n",
    "\n",
    "The processing steps for the fMRI broadly include:\n",
    "* Slice-time correction\n",
    "* Rigid realignment\n",
    "* Co-registration to the sMRI (T2-weighted structural MRI)\n",
    "* Artifact detection:\n",
    "    - Motion\n",
    "    - Global intensity outliers\n",
    "* De-noising to remove:\n",
    "    - Component noise associated with white matter and CSF\n",
    "    - component noise associated with motion\n",
    "    - Censoring/scrubbing of individual volumes detected as artifacts in the previous step\n",
    "    - Frame-wise displacement\n",
    "* Bandpass filtering\n",
    "* Spatial smoothing\n",
    "* Registration to infant sample template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/iang/active/BABIES/BABIES_rest/BABIES_rest/misc/all_subjects.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d1ca8c5fad31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtemplate_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudyhome\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/templates/T2w_BABIES_template_2mm_mask.nii.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0msubjects_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudyhome\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/BABIES_rest/misc/all_subjects.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;31m#subjects_list = ['sham1','sham2']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/iang/active/BABIES/BABIES_rest/BABIES_rest/misc/all_subjects.txt'"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "from os import listdir, makedirs\n",
    "from os.path import isdir\n",
    "from nipype.interfaces.io import DataSink, SelectFiles, DataGrabber # Data i/o\n",
    "from nipype.interfaces.utility import IdentityInterface, Function     # utility\n",
    "from nipype.pipeline.engine import Node, Workflow, MapNode        # pypeline engine\n",
    "from nipype.interfaces.nipy.preprocess import Trim\n",
    "\n",
    "from nipype.algorithms.rapidart import ArtifactDetect\n",
    "from nipype.interfaces.fsl.preprocess import SliceTimer, MCFLIRT, FLIRT, FAST, SUSAN\n",
    "from nipype.interfaces.fsl.utils import Reorient2Std, MotionOutliers, Merge\n",
    "from nipype.interfaces.fsl.model import GLM\n",
    "from nipype.interfaces.fsl.maths import ApplyMask, TemporalFilter\n",
    "from nipype.interfaces.freesurfer import Resample, Binarize, MRIConvert\n",
    "from nipype.algorithms.confounds import CompCor\n",
    "from nipype.interfaces.afni.preprocess import Bandpass\n",
    "from nipype.interfaces.afni.utils import AFNItoNIFTI\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "#set output file type for FSL to NIFTI\n",
    "from nipype.interfaces.fsl.preprocess import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI_GZ')\n",
    "\n",
    "# MATLAB setup - Specify path to current SPM and the MATLAB's default mode\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('~/spm12')\n",
    "MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "\n",
    "# Set study variables\n",
    "setup='myelin'\n",
    "\n",
    "if setup == 'myelin':\n",
    "    studyhome = '/Volumes/iang/active/BABIES/BABIES_rest'\n",
    "    raw_data = studyhome + 'subjDir/'\n",
    "    output_dir = studyhome + '/processed/preproc'\n",
    "    workflow_dir = studyhome + '/workflows'\n",
    "    template_brain = studyhome + '/templates/T2w_BABIES_template_2mm.nii.gz'\n",
    "    template_wm = studyhome + '/templates/BABIES_wm_mask_2mm.nii.gz'\n",
    "    template_mask = studyhome + '/templates/T2w_BABIES_template_2mm_mask.nii.gz'\n",
    "\n",
    "if setup == 'Lucy':\n",
    "    studyhome = '/share/iang/active/BABIES/BABIES_rest/'\n",
    "    raw_data = studyhome + '/subjDir/'\n",
    "    output_dir = studyhome + '/processed/preproc'\n",
    "    workflow_dir = studyhome + '/workflows'\n",
    "    template_brain = studyhome + '/templates/T2w_BABIES_template_2mm.nii.gz'\n",
    "    template_wm = studyhome + '/templates/BABIES_wm_mask_2mm.nii.gz'\n",
    "    template_mask = studyhome + '/templates/T2w_BABIES_template_2mm_mask.nii.gz'\n",
    "    \n",
    "elif setup =='Cat':\n",
    "    studyhome = '/Users/catcamacho/Box/SNAP/BABIES'\n",
    "    raw_data = studyhome + '/raw'\n",
    "    output_dir = studyhome + '/BABIES_rest/processed/preproc'\n",
    "    workflow_dir = studyhome + '/BABIES_rest/workflows'\n",
    "    template_brain = studyhome + '/templates/T2w_BABIES_template_2mm.nii.gz'\n",
    "    template_wm = studyhome + '/templates/BABIES_wm_mask_2mm.nii.gz'\n",
    "    template_mask = studyhome + '/templates/T2w_BABIES_template_2mm_mask.nii.gz'\n",
    "\n",
    "subjects_list = open(studyhome + '/BABIES_rest/misc/all_subjects.txt').read().splitlines()\n",
    "#subjects_list = ['sham1','sham2']\n",
    "\n",
    "proc_cores = 2 # number of cores of processing for the workflows\n",
    "vols_to_trim = 4\n",
    "interleave = False\n",
    "TR = 2.5 # in seconds\n",
    "slice_dir = 3 # 1=x, 2=y, 3=z\n",
    "resampled_voxel_size = (2,2,2)\n",
    "fwhm = 4 #fwhm for smoothing with SUSAN\n",
    "\n",
    "#changed to match Pendl et al 2017 (HBM)\n",
    "highpass_freq = 0.08 #in Hz\n",
    "lowpass_freq = 0.1 #in Hz\n",
    "\n",
    "mask_erosion = 1\n",
    "mask_dilation = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File handling Nodes\n",
    "\n",
    "# Identity node- select subjects\n",
    "infosource = Node(IdentityInterface(fields=['subject_id']),\n",
    "                     name='infosource')\n",
    "infosource.iterables = ('subject_id', subjects_list)\n",
    "\n",
    "# Datasink- where our select outputs will go\n",
    "substitutions = [('_subject_id_', '')]\n",
    "datasink = Node(DataSink(), name='datasink')\n",
    "datasink.inputs.base_directory = output_dir\n",
    "datasink.inputs.container = output_dir\n",
    "datasink.inputs.substitutions = substitutions\n",
    "\n",
    "# Data grabber- select fMRI and sMRI\n",
    "if setup=='myelin':\n",
    "    anat_template = {'struct': raw_data + '{subject_id}/skullstripped_anat.nii.gz'}\n",
    "    select_anat = Node(SelectFiles(anat_template), name='selectanat')\n",
    "\n",
    "    func_template = {'func': raw_data + '{subject_id}/rest*.nii.gz'}\n",
    "    select_func = Node(DataGrabber(sort_filelist=True,\n",
    "                                   template = raw_data + '%s/rest*.nii.gz',\n",
    "                                   field_template = func_template,\n",
    "                                   base_directory=raw_data,\n",
    "                                   infields=['subject_id'],\n",
    "                                   template_args={'func':[['subject_id']]}),\n",
    "                       name='select_func') \n",
    "\n",
    "if setup=='Lucy':\n",
    "    anat_template = {'struct': raw_data + '{subject_id}/skullstripped_anat.nii.gz'}\n",
    "    select_anat = Node(SelectFiles(anat_template), name='selectanat')\n",
    "\n",
    "    func_template = {'func': raw_data + '{subject_id}/rest*.nii.gz'}\n",
    "    select_func = Node(DataGrabber(sort_filelist=True,\n",
    "                                   template = raw_data + '%s/rest*.nii.gz',\n",
    "                                   field_template = func_template,\n",
    "                                   base_directory=raw_data,\n",
    "                                   infields=['subject_id'],\n",
    "                                   template_args={'func':[['subject_id']]}),\n",
    "                       name='select_func') \n",
    "    \n",
    "elif setup=='Cat':\n",
    "    anat_template = {'struct': raw_data + '/{subject_id}/skullstripped_anat.nii.gz'}\n",
    "    select_anat = Node(SelectFiles(anat_template), name='selectanat')\n",
    "\n",
    "    func_template = {'func': raw_data + '/%s/rest_raw*.nii.gz'}\n",
    "    select_func = Node(DataGrabber(sort_filelist=True,\n",
    "                                   template = raw_data + '/%s/rest_raw*.nii.gz',\n",
    "                                   field_template = func_template,\n",
    "                                   base_directory=raw_data,\n",
    "                                   infields=['subject_id'], \n",
    "                                   template_args={'func':[['subject_id']]}), \n",
    "                       name='select_func')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## struct processing\n",
    "# reorient to standard\n",
    "reorientstruct = Node(Reorient2Std(), name='reorientstruct')\n",
    "\n",
    "# convert files to nifti\n",
    "reslice_struct = Node(MRIConvert(out_type='niigz',\n",
    "                                 conform_size=2,\n",
    "                                 crop_size=(128, 128, 128),\n",
    "                                ),\n",
    "                   name='reslice_struct')\n",
    "# Segment structural scan\n",
    "segment = Node(FAST(no_bias=True, \n",
    "                    segments=True, \n",
    "                    number_classes=3), \n",
    "               name='segment')\n",
    "\n",
    "# register BOLD to anat\n",
    "coregT2 = Node(FLIRT(out_matrix_file='xform.mat'),\n",
    "           name='coregT2')\n",
    "\n",
    "# apply transform to func\n",
    "applyT2xform = Node(FLIRT(apply_xfm=True),\n",
    "                    name='applyT2xform')\n",
    "\n",
    "# register anat to template\n",
    "reg_temp = Node(FLIRT(reference=template_brain, \n",
    "                      out_matrix_file='xform.mat',\n",
    "                      out_file='preproc_anat.nii.gz'),\n",
    "                name='reg_temp')\n",
    "\n",
    "# apply transform to func\n",
    "applyxform = Node(FLIRT(reference=template_brain,\n",
    "                        apply_xfm=True, \n",
    "                        out_file='preproc_func.nii.gz'),\n",
    "                  name='applyxform')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_fd(fd_list):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from os.path import abspath\n",
    "    from numpy import asarray, savetxt\n",
    "    \n",
    "    motion = open(fd_list[0]).read().splitlines()\n",
    "\n",
    "    if len(fd_list)>1:\n",
    "        for file in fd_list[1:]:\n",
    "            temp = open(file).read().splitlines()\n",
    "            motion = motion+temp\n",
    "\n",
    "    motion = asarray(motion).astype(float)\n",
    "    filename = 'FD_full.txt'\n",
    "    savetxt(filename,motion)\n",
    "    combined_fd = abspath(filename)\n",
    "    return(combined_fd)\n",
    "\n",
    "\n",
    "def combine_par(par_list):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from os.path import abspath\n",
    "    from numpy import vstack, savetxt, genfromtxt\n",
    "    \n",
    "    motion = genfromtxt(par_list[0], dtype=float)\n",
    "    if len(par_list)>1:\n",
    "        for file in par_list[1:]:\n",
    "            temp = genfromtxt(par_list[0], dtype=float)\n",
    "            motion=vstack((motion,temp))\n",
    "    \n",
    "    filename = 'motion.par'\n",
    "    savetxt(filename, motion, delimiter=' ')\n",
    "    combined_par = abspath(filename)\n",
    "    return(combined_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data QC nodes\n",
    "def create_coreg_plot(epi,anat):\n",
    "    import os\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from nilearn import plotting\n",
    "    \n",
    "    coreg_filename='coregistration.png'\n",
    "    display = plotting.plot_anat(epi, display_mode='ortho',\n",
    "                                 draw_cross=False,\n",
    "                                 title = 'coregistration to anatomy')\n",
    "    display.add_edges(anat)\n",
    "    display.savefig(coreg_filename) \n",
    "    display.close()\n",
    "    coreg_file = os.path.abspath(coreg_filename)\n",
    "    \n",
    "    return(coreg_file)\n",
    "\n",
    "def check_mask_coverage(epi,brainmask):\n",
    "    from os.path import abspath\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from nilearn import plotting\n",
    "    from nipype.interfaces.nipy.preprocess import Trim\n",
    "    \n",
    "    trim = Trim()\n",
    "    trim.inputs.in_file = epi\n",
    "    trim.inputs.end_index = 1\n",
    "    trim.inputs.out_file = 'epi_vol1.nii.gz'\n",
    "    trim.run()\n",
    "    epi_vol = abspath('epi_vol1.nii.gz')\n",
    "    \n",
    "    maskcheck_filename='maskcheck.png'\n",
    "    display = plotting.plot_anat(epi_vol, display_mode='ortho',\n",
    "                                 draw_cross=False,\n",
    "                                 title = 'brainmask coverage')\n",
    "    display.add_contours(brainmask,levels=[.5], colors='r')\n",
    "    display.savefig(maskcheck_filename)\n",
    "    display.close()\n",
    "    maskcheck_file = abspath(maskcheck_filename)\n",
    "\n",
    "    return(maskcheck_file)\n",
    "\n",
    "make_coreg_img = Node(Function(input_names=['epi','anat'],\n",
    "                               output_names=['coreg_file'],\n",
    "                               function=create_coreg_plot), name='make_coreg_img')\n",
    "\n",
    "make_checkmask_img = Node(\n",
    "    Function(input_names=['epi','brainmask'],\n",
    "             output_names=['maskcheck_file'],\n",
    "             function=check_mask_coverage), name='make_checkmask_img')\n",
    "make_checkmask_img.inputs.brainmask = template_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Nodes for func preprocessing\n",
    "\n",
    "# Reorient to standard space using FSL\n",
    "reorientfunc = MapNode(Reorient2Std(), name='reorientfunc', iterfield=['in_file'])\n",
    "\n",
    "# Trim first 4 volumes\n",
    "trimvols = MapNode(Trim(begin_index=vols_to_trim), name='trimvols', iterfield=['in_file'])\n",
    "\n",
    "# Motion correction\n",
    "motion_correct = MapNode(MCFLIRT(save_plots=True, mean_vol=True), name='motion_correct', iterfield=['in_file'])\n",
    "\n",
    "# Get frame-wise displacement for each run: in_file; out_file, out_metric_plot, out_metric_values\n",
    "get_FD = MapNode(MotionOutliers(metric = 'fd',\n",
    "                                out_metric_values = 'FD.txt',\n",
    "                                out_metric_plot = 'motionplot.png',\n",
    "                                no_motion_correction=False,\n",
    "                                threshold=0.01),\n",
    "                    name='get_FD', iterfield=['in_file'])\n",
    "\n",
    "# Merge rest\n",
    "merge = Node(Merge(dimension='t'), name='merge')\n",
    "\n",
    "# Merge motion (6 params)\n",
    "comb_par = Node(Function(input_names=['par_list'],\n",
    "                         output_names=['combined_par'],\n",
    "                         function=combine_par), name='comb_par')\n",
    "\n",
    "# Merge FD\n",
    "comb_fd = Node(Function(input_names=['fd_list'],\n",
    "                         output_names=['combined_fd'],\n",
    "                         function=combine_fd), name='comb_fd')\n",
    "\n",
    "# Rigid body realignment\n",
    "realignment = Node(MCFLIRT(), name='realignment')\n",
    "\n",
    "#Slice timing correction based on interleaved acquisition using FSL\n",
    "slicetime_correct = Node(SliceTimer(interleaved=interleave, \n",
    "                                    slice_direction=slice_dir,\n",
    "                                    time_repetition=TR, \n",
    "                                    out_file='st_func.nii.gz'),\n",
    "                         name='slicetime_correct')\n",
    "\n",
    "# unzip the nifti for ART\n",
    "gunzip = Node(Gunzip(), name='gunzip')\n",
    "\n",
    "# Artifact detection for scrubbing/motion assessment\n",
    "art = Node(ArtifactDetect(mask_type='file',\n",
    "                          parameter_source='FSL',\n",
    "                          norm_threshold=0.25, #mutually exclusive with rotation and translation thresh\n",
    "                          zintensity_threshold=2,\n",
    "                          use_differences=[True, False], \n",
    "                          mask_file=template_mask),\n",
    "           name='art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180811-12:20:38,65 workflow INFO:\n",
      "\t Generated workflow graph: /Volumes/iang/active/BABIES/BABIES_rest/workflows/preprocwf/graph.dot.png (graph2use=flat, simple_form=True).\n",
      "180811-12:20:38,142 workflow INFO:\n",
      "\t Workflow preprocwf settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180811-12:20:39,384 workflow INFO:\n",
      "\t Running in parallel.\n",
      "180811-12:20:39,394 workflow INFO:\n",
      "\t Currently running 0 tasks, and 4 jobs ready. Free memory (GB): 7.20/7.20, Free processors: 2/2\n",
      "180811-12:20:39,511 workflow INFO:\n",
      "\t Executing node preprocwf.selectanat in dir: /Volumes/iang/active/BABIES/BABIES_rest/workflows/preprocwf/_subject_id_sham2/selectanat\n",
      "180811-12:20:39,601 workflow INFO:\n",
      "\t Executing node preprocwf.select_func in dir: /Volumes/iang/active/BABIES/BABIES_rest/workflows/preprocwf/_subject_id_sham2/select_func\n",
      "180811-12:20:41,600 workflow INFO:\n",
      "\t Currently running 2 tasks, and 2 jobs ready. Free memory (GB): 6.80/7.20, Free processors: 0/2\n",
      "180811-12:20:42,40 workflow INFO:\n",
      "\t Running node \"selectanat\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180811-12:20:42,547 workflow INFO:\n",
      "\t Running node \"select_func\" (\"nipype.interfaces.io.DataGrabber\").\n",
      "180811-12:20:43,607 workflow ERROR:\n",
      "\t Node selectanat.a1 failed to run on host Lucindas-MacBook-Pro.local.\n",
      "180811-12:20:43,613 workflow ERROR:\n",
      "\t Saving crash info to /Users/lucindasisk/Dropbox/Github/infant_rest/crash-20180811-122043-lucindasisk-selectanat.a1-ce1eb404-a622-4e19-9be6-5b2621d94e2c.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1090, in run\n",
      "    outputs = self.aggregate_outputs(runtime)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1155, in aggregate_outputs\n",
      "    predicted_outputs = self._list_outputs()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/io.py\", line 1323, in _list_outputs\n",
      "    raise IOError(msg)\n",
      "OSError: No files were found matching struct template: /Volumes/iang/active/BABIES/BABIES_rest/subjDir/sham2/skullstripped_anat.nii.gz\n",
      "\n",
      "180811-12:20:43,620 workflow ERROR:\n",
      "\t Node select_func.a1 failed to run on host Lucindas-MacBook-Pro.local.\n",
      "180811-12:20:43,623 workflow ERROR:\n",
      "\t Saving crash info to /Users/lucindasisk/Dropbox/Github/infant_rest/crash-20180811-122043-lucindasisk-select_func.a1-bef0d01c-9dea-42af-940a-95292b9e84c5.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1090, in run\n",
      "    outputs = self.aggregate_outputs(runtime)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1155, in aggregate_outputs\n",
      "    predicted_outputs = self._list_outputs()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/io.py\", line 1168, in _list_outputs\n",
      "    raise IOError(msg)\n",
      "OSError: Output key: func Template: /Volumes/iang/active/BABIES/BABIES_rest/subjDir/sham2/rest*.nii.gz returned no files\n",
      "\n",
      "180811-12:20:43,631 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 7.20/7.20, Free processors: 2/2\n",
      "180811-12:20:43,782 workflow INFO:\n",
      "\t Executing node preprocwf.selectanat in dir: /Volumes/iang/active/BABIES/BABIES_rest/workflows/preprocwf/_subject_id_sham1/selectanat\n",
      "180811-12:20:43,924 workflow INFO:\n",
      "\t Executing node preprocwf.select_func in dir: /Volumes/iang/active/BABIES/BABIES_rest/workflows/preprocwf/_subject_id_sham1/select_func\n",
      "180811-12:20:45,930 workflow INFO:\n",
      "\t Currently running 2 tasks, and 0 jobs ready. Free memory (GB): 6.80/7.20, Free processors: 0/2\n",
      "180811-12:20:46,260 workflow INFO:\n",
      "\t Running node \"selectanat\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "180811-12:20:46,430 workflow INFO:\n",
      "\t Running node \"select_func\" (\"nipype.interfaces.io.DataGrabber\").\n",
      "180811-12:20:47,938 workflow ERROR:\n",
      "\t Node selectanat.a0 failed to run on host Lucindas-MacBook-Pro.local.\n",
      "180811-12:20:47,942 workflow ERROR:\n",
      "\t Saving crash info to /Users/lucindasisk/Dropbox/Github/infant_rest/crash-20180811-122047-lucindasisk-selectanat.a0-f372ce48-9d99-4e0a-8c3a-ae2054bb868c.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1090, in run\n",
      "    outputs = self.aggregate_outputs(runtime)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1155, in aggregate_outputs\n",
      "    predicted_outputs = self._list_outputs()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/io.py\", line 1323, in _list_outputs\n",
      "    raise IOError(msg)\n",
      "OSError: No files were found matching struct template: /Volumes/iang/active/BABIES/BABIES_rest/subjDir/sham1/skullstripped_anat.nii.gz\n",
      "\n",
      "180811-12:20:47,948 workflow ERROR:\n",
      "\t Node select_func.a0 failed to run on host Lucindas-MacBook-Pro.local.\n",
      "180811-12:20:47,959 workflow ERROR:\n",
      "\t Saving crash info to /Users/lucindasisk/Dropbox/Github/infant_rest/crash-20180811-122047-lucindasisk-select_func.a0-cd9527fd-45cf-4524-a36e-ebc14851fbd4.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1090, in run\n",
      "    outputs = self.aggregate_outputs(runtime)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1155, in aggregate_outputs\n",
      "    predicted_outputs = self._list_outputs()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/io.py\", line 1168, in _list_outputs\n",
      "    raise IOError(msg)\n",
      "OSError: Output key: func Template: /Volumes/iang/active/BABIES/BABIES_rest/subjDir/sham1/rest*.nii.gz returned no files\n",
      "\n",
      "180811-12:20:47,972 workflow INFO:\n",
      "\t Currently running 0 tasks, and 0 jobs ready. Free memory (GB): 7.20/7.20, Free processors: 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180811-12:20:49,981 workflow INFO:\n",
      "\t ***********************************\n",
      "180811-12:20:49,985 workflow ERROR:\n",
      "\t could not run node: preprocwf.selectanat.a1\n",
      "180811-12:20:49,987 workflow INFO:\n",
      "\t crashfile: /Users/lucindasisk/Dropbox/Github/infant_rest/crash-20180811-122043-lucindasisk-selectanat.a1-ce1eb404-a622-4e19-9be6-5b2621d94e2c.pklz\n",
      "180811-12:20:49,990 workflow ERROR:\n",
      "\t could not run node: preprocwf.select_func.a1\n",
      "180811-12:20:49,992 workflow INFO:\n",
      "\t crashfile: /Users/lucindasisk/Dropbox/Github/infant_rest/crash-20180811-122043-lucindasisk-select_func.a1-bef0d01c-9dea-42af-940a-95292b9e84c5.pklz\n",
      "180811-12:20:49,995 workflow ERROR:\n",
      "\t could not run node: preprocwf.selectanat.a0\n",
      "180811-12:20:49,996 workflow INFO:\n",
      "\t crashfile: /Users/lucindasisk/Dropbox/Github/infant_rest/crash-20180811-122047-lucindasisk-selectanat.a0-f372ce48-9d99-4e0a-8c3a-ae2054bb868c.pklz\n",
      "180811-12:20:49,999 workflow ERROR:\n",
      "\t could not run node: preprocwf.select_func.a0\n",
      "180811-12:20:50,2 workflow INFO:\n",
      "\t crashfile: /Users/lucindasisk/Dropbox/Github/infant_rest/crash-20180811-122047-lucindasisk-select_func.a0-cd9527fd-45cf-4524-a36e-ebc14851fbd4.pklz\n",
      "180811-12:20:50,7 workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-223066e4214e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mpreprocwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkflow_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mpreprocwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph2use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mpreprocwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MultiProc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_procs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mproc_cores\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_node_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# close any open resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         raise RuntimeError(('Workflow did not execute cleanly. '\n\u001b[0m\u001b[1;32m     80\u001b[0m                             'Check log for details'))\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process NonDaemonPoolWorker-6:\n",
      "Process NonDaemonPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing Workflow\n",
    "\n",
    "# workflowname.connect([(node1,node2,[('node1output','node2input')]),\n",
    "#                    (node2,node3,[('node2output','node3input')])\n",
    "#                    ])\n",
    "\n",
    "preprocwf = Workflow(name='preprocwf')\n",
    "preprocwf.connect([(infosource,select_func,[('subject_id','subject_id')]), \n",
    "                   (select_func,reorientfunc,[('func','in_file')]),\n",
    "                   (reorientfunc,trimvols,[('out_file','in_file')]),\n",
    "                   (trimvols,motion_correct,[('out_file','in_file')]),\n",
    "                   (trimvols,get_FD,[('out_file','in_file')]),\n",
    "                   (motion_correct,merge,[('out_file','in_files')]),\n",
    "                   (merge,realignment,[('merged_file','in_file')]),\n",
    "                   (realignment,slicetime_correct,[('out_file','in_file')]),\n",
    "                   (get_FD,comb_fd,[('out_metric_values','fd_list')]),\n",
    "                   (motion_correct,comb_par,[('par_file','par_list')]),\n",
    "                   (comb_par,art,[('combined_par','realignment_parameters')]),\n",
    "                   (applyxform,gunzip,[('out_file','in_file')]),\n",
    "                   (gunzip,art,[('out_file','realigned_files')]),\n",
    "                   \n",
    "                   (infosource,select_anat,[('subject_id','subject_id')]),\n",
    "                   (select_anat,reorientstruct,[('struct','in_file')]),\n",
    "                   (reorientstruct,reslice_struct,[('out_file','in_file')]),\n",
    "                   (reslice_struct,coregT2,[('out_file','reference')]),\n",
    "                   (reslice_struct,applyT2xform,[('out_file','reference')]),\n",
    "                   (reslice_struct,reg_temp,[('out_file','in_file')]),\n",
    "                   (reg_temp,applyxform,[('out_matrix_file','in_matrix_file')]),\n",
    "                   (slicetime_correct,coregT2,[('slice_time_corrected_file','in_file')]),\n",
    "                   (slicetime_correct,applyT2xform,[('slice_time_corrected_file','in_file')]),\n",
    "                   (coregT2,applyT2xform,[('out_matrix_file','in_matrix_file')]),\n",
    "                   (applyT2xform, applyxform,[('out_file','in_file')]),\n",
    "                   (reg_temp,segment,[('out_file','in_files')]),\n",
    "                   \n",
    "                   (coregT2,make_coreg_img,[('out_file','epi')]),\n",
    "                   (applyxform,make_checkmask_img,[('out_file','epi')]),\n",
    "                   (reslice_struct,make_coreg_img,[('out_file','anat')]),\n",
    "                   \n",
    "                   (applyxform, datasink, [('out_file','preproc_func')]),\n",
    "                   (reg_temp, datasink,[('out_file','preproc_anat')]),\n",
    "                   (comb_fd, datasink, [('combined_fd','FD_out_metric_values')]),\n",
    "                   (comb_par,datasink,[('combined_par','motion_params')]),\n",
    "                   (segment,datasink,[('tissue_class_files','tissue_class_files')]),\n",
    "                   (art,datasink, [('plot_files','art_plot_files')]),\n",
    "                   (art,datasink, [('outlier_files','vols_to_censor')]),\n",
    "                   (make_checkmask_img,datasink,[('maskcheck_file','maskcheck_image')]),\n",
    "                   (make_coreg_img,datasink,[('coreg_file','coreg_image')])                   \n",
    "                  ])\n",
    "preprocwf.base_dir = workflow_dir\n",
    "preprocwf.write_graph(graph2use='flat')\n",
    "preprocwf.run('MultiProc', plugin_args={'n_procs': proc_cores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resting state preprocessing\n",
    "# Identity node- select subjects\n",
    "infosource = Node(IdentityInterface(fields=['subject_id']),\n",
    "                     name='infosource')\n",
    "infosource.iterables = ('subject_id', subjects_list)\n",
    "\n",
    "\n",
    "# Data grabber- select fMRI and sMRI\n",
    "templates = {'func': output_dir + '/preproc_func/{subject_id}/preproc_func.nii.gz',\n",
    "             'csf': output_dir + '/tissue_class_files/{subject_id}/preproc_anat_seg_0.nii.gz', \n",
    "             'vols_to_censor':output_dir + '/vols_to_censor/{subject_id}/art.preproc_func_outliers.txt', \n",
    "             'motion_params':output_dir + '/FD_out_metric_values/{subject_id}/FD_full.txt',\n",
    "             'wm':template_wm}\n",
    "selectfiles = Node(SelectFiles(templates), name='selectfiles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pull motion info for all subjects\n",
    "\n",
    "motion_df_file = output_dir + '/motion_summary/motionSummary.csv'\n",
    "\n",
    "if isdir(output_dir + '/motion_summary')==False:\n",
    "    makedirs(output_dir + '/motion_summary')\n",
    "    motion_df = DataFrame(columns=['meanFD','maxFD','NumCensoredVols','totalVolumes','secondsNotCensored','lengthsNotCensored_descendingSeconds'])\n",
    "    motion_df.to_csv(motion_df_file)\n",
    "    \n",
    "def summarize_motion(motion_df_file, motion_file, vols_to_censor, TR):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from os.path import dirname, basename\n",
    "    from numpy import asarray, mean, insert, zeros, sort\n",
    "    from pandas import DataFrame, Series, read_csv\n",
    "    \n",
    "    motion_df = read_csv(motion_df_file, index_col=0)\n",
    "    \n",
    "    motion = asarray(open(motion_file).read().splitlines()).astype(float)\n",
    "    censvols = asarray(open(vols_to_censor).read().splitlines()).astype(int)\n",
    "    sec_not_censored = (len(motion)-len(censvols))*TR\n",
    "    \n",
    "    if censvols[0]>0:\n",
    "        periods_not_censored = insert(censvols,0,0)\n",
    "    else:\n",
    "        periods_not_censored = censvols\n",
    "    \n",
    "    if periods_not_censored[-1]<len(motion):\n",
    "        periods_not_censored = insert(periods_not_censored,len(periods_not_censored),len(motion))\n",
    "    \n",
    "    lengths = zeros(len(periods_not_censored)-1)\n",
    "    for a in range(0,len(lengths)):\n",
    "        lengths[a] = periods_not_censored[a+1] - periods_not_censored[a] - 1\n",
    "    \n",
    "    lengths = lengths*TR\n",
    "    \n",
    "    # sort lengths in descending order\n",
    "    lengths = sort(lengths)[::-1]\n",
    "\n",
    "    fp = dirname(motion_file)\n",
    "    subject = basename(fp)\n",
    "\n",
    "    motion_df.loc[subject] = [mean(motion),max(motion),len(censvols),len(motion),sec_not_censored,lengths]\n",
    "    motion_df.to_csv(motion_df_file)\n",
    "\n",
    "    return()\n",
    "\n",
    "# Make a list of tissues for component noise removal\n",
    "def combine_masks(mask1,mask2):\n",
    "    from nipype.interfaces.fsl.utils import Merge\n",
    "    from os.path import abspath\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    \n",
    "    vols = []\n",
    "    vols.append(mask1)\n",
    "    vols.append(mask2)\n",
    "    \n",
    "    return(vols)\n",
    "    \n",
    "# Remove all noise (GLM with noise params)\n",
    "def create_noise_matrix(vols_to_censor,motion_params,comp_noise):\n",
    "    from numpy import genfromtxt, zeros,concatenate, savetxt\n",
    "    from os import path\n",
    "\n",
    "    motion = genfromtxt(motion_params, delimiter=' ', dtype=None, skip_header=0)\n",
    "    comp_noise = genfromtxt(comp_noise, delimiter='\\t', dtype=None, skip_header=1)\n",
    "    censor_vol_list = genfromtxt(vols_to_censor, delimiter='\\t', dtype=None, skip_header=0)\n",
    "\n",
    "    c = len(censor_vol_list)\n",
    "    d = len(comp_noise)\n",
    "    if c > 0:\n",
    "        scrubbing = zeros((d,c),dtype=int)\n",
    "        for t in range(0,c):\n",
    "            scrubbing[censor_vol_list[t]][t] = 1    \n",
    "        noise_matrix = concatenate([motion[:,None],comp_noise,scrubbing],axis=1)\n",
    "    else:\n",
    "        noise_matrix = concatenate((motion[:,None],comp_noise),axis=1)\n",
    "\n",
    "    noise_file = 'noise_matrix.txt'\n",
    "    savetxt(noise_file, noise_matrix, delimiter='\\t')\n",
    "    noise_filepath = path.abspath(noise_file)\n",
    "    \n",
    "    return(noise_filepath)\n",
    "\n",
    "def convertafni(in_file):\n",
    "    from nipype.interfaces.afni.utils import AFNItoNIFTI\n",
    "    from os import path\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    \n",
    "    cvt = AFNItoNIFTI()\n",
    "    cvt.inputs.in_file = in_file\n",
    "    cvt.inputs.out_file = 'func_filtered.nii.gz'\n",
    "    cvt.run()\n",
    "    \n",
    "    out_file = path.abspath('func_filtered.nii.gz')\n",
    "    return(out_file)\n",
    "\n",
    "# Brightness threshold should be 0.75 * the contrast between the median brain intensity and the background\n",
    "def brightthresh(func):\n",
    "    import nibabel as nib\n",
    "    from numpy import median, where\n",
    "    \n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    \n",
    "    func_nifti1 = nib.load(func)\n",
    "    func_data = func_nifti1.get_data()\n",
    "    func_data = func_data.astype(float)\n",
    "    \n",
    "    brain_values = where(func_data > 0)\n",
    "    median_thresh = median(brain_values)\n",
    "    bright_thresh = 0.75 * median_thresh\n",
    "    \n",
    "    return(bright_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Denoising\n",
    "merge_confs = Node(Function(input_names=['mask1','mask2'],\n",
    "                            output_names=['vols'], \n",
    "                            function=combine_masks), \n",
    "                   name='merge_confs')\n",
    "\n",
    "compcor = Node(CompCor(merge_method='none'), \n",
    "               name='compcor')\n",
    "\n",
    "noise_mat = Node(Function(input_names=['vols_to_censor','motion_params','comp_noise'],\n",
    "                          output_names=['noise_filepath'], \n",
    "                          function=create_noise_matrix), \n",
    "                 name='noise_mat')\n",
    "\n",
    "denoise = Node(GLM(out_res_name='denoised_residuals.nii.gz', \n",
    "                   out_data_name='denoised_func.nii.gz'), \n",
    "               name='denoise')\n",
    "\n",
    "# band pass filtering- all rates are in Hz (1/TR or samples/second)\n",
    "bandpass = Node(Bandpass(highpass=highpass_freq,\n",
    "                         lowpass=lowpass_freq), \n",
    "                name='bandpass')\n",
    "\n",
    "afni_convert = Node(Function(input_names=['in_file'],\n",
    "                             output_names=['out_file'],\n",
    "                             function=convertafni), \n",
    "                    name='afni_convert')\n",
    "\n",
    "# Spatial smoothing \n",
    "brightthresh_filt = Node(Function(input_names=['func'], \n",
    "                                  output_names=['bright_thresh'], \n",
    "                                  function=brightthresh), \n",
    "                         name='brightthresh_filt')    \n",
    "    \n",
    "smooth_filt = Node(SUSAN(fwhm=fwhm), name='smooth_filt')\n",
    "\n",
    "motion_summary = Node(Function(input_names=['motion_df_file','motion_file','vols_to_censor'], \n",
    "                               output_names=[], \n",
    "                               function=summarize_motion), \n",
    "                      name='motion_summary')\n",
    "motion_summary.inputs.motion_df_file = motion_df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# workflowname.connect([(node1,node2,[('node1output','node2input')]),\n",
    "#                       (node2,node3,[('node2output','node3input')])\n",
    "#                     ])\n",
    "\n",
    "rs_procwf = Workflow(name='rs_procwf')\n",
    "rs_procwf.connect([(infosource,selectfiles,[('subject_id','subject_id')]),\n",
    "                   (selectfiles,compcor,[('func','realigned_file')]),\n",
    "                   (selectfiles,merge_confs,[('csf','mask1')]),\n",
    "                   (selectfiles,merge_confs,[('wm','mask2')]),\n",
    "                   (merge_confs,compcor,[('vols','mask_files')]),\n",
    "                   (compcor,noise_mat,[('components_file','comp_noise')]),\n",
    "                   (selectfiles,noise_mat,[('vols_to_censor','vols_to_censor'),\n",
    "                                           ('motion_params','motion_params')]),\n",
    "                   (noise_mat,denoise,[('noise_filepath','design')]),\n",
    "                   (selectfiles,denoise,[('func','in_file')]),\n",
    "                   (denoise,bandpass,[('out_data','in_file')]),\n",
    "                   (bandpass,afni_convert,[('out_file','in_file')]),\n",
    "                   (afni_convert,brightthresh_filt,[('out_file','func')]),\n",
    "                   (brightthresh_filt,smooth_filt,[('bright_thresh','brightness_threshold')]),\n",
    "                   (afni_convert,smooth_filt,[('out_file','in_file')]),  \n",
    "                   (selectfiles, motion_summary, [('motion_params','motion_file'),\n",
    "                                                  ('vols_to_censor','vols_to_censor')]),\n",
    "                   \n",
    "                   (smooth_filt,datasink,[('smoothed_file','proc_func')])\n",
    "                   ])\n",
    "\n",
    "rs_procwf.base_dir = workflow_dir\n",
    "rs_procwf.write_graph(graph2use='flat')\n",
    "rs_procwf.run('MultiProc', plugin_args={'n_procs': proc_cores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
