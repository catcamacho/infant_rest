{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "from nipype.interfaces.io import DataSink, SelectFiles, DataGrabber # Data i/o\n",
    "from nipype.interfaces.utility import IdentityInterface, Function     # utility\n",
    "from nipype.pipeline.engine import Node, Workflow, JoinNode,MapNode      # pypeline engine\n",
    "\n",
    "from nipype.interfaces.fsl.model import Randomise, GLM, Cluster\n",
    "from nipype.interfaces.freesurfer.model import Binarize\n",
    "from nipype.interfaces.fsl.utils import ImageMeants, Merge, Split\n",
    "from nipype.interfaces.fsl.maths import ApplyMask\n",
    "\n",
    "#set output file type for FSL to NIFTI\n",
    "from nipype.interfaces.fsl.preprocess import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI')\n",
    "\n",
    "# MATLAB setup - Specify path to current SPM and the MATLAB's default mode\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('~/spm12')\n",
    "MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "\n",
    "# Set study variables\n",
    "#studyhome = '/Users/catcamacho/Box/SNAP/BABIES'\n",
    "studyhome = '/Volumes/iang/active/BABIES/BABIES_rest'\n",
    "raw_data = studyhome + '/raw'\n",
    "preproc_dir = studyhome + '/processed/preproc'\n",
    "output_dir = studyhome + '/processed/sbc_analysis'\n",
    "workflow_dir = studyhome + '/workflows'\n",
    "roi_dir = studyhome + '/ROIs'\n",
    "group_con = studyhome + '/misc/tcon.con'\n",
    "group_mat = studyhome + '/misc/design.mat'\n",
    "proc_cores = 2\n",
    "\n",
    "subjects_list = ['021-BABIES-T1']\n",
    "#subjects_list = open(studyhome + '/misc/subjects.txt').read().splitlines()\n",
    "\n",
    "template_brain = studyhome + '/templates/T2w_BABIES_template_2mm.nii'\n",
    "gm_template = studyhome + '/templates/BABIES_gm_mask_2mm.nii'\n",
    "\n",
    "# ROIs for connectivity analysis\n",
    "Lamyg = roi_dir + '/L_amyg_4mm.nii'\n",
    "Ramyg = roi_dir + '/R_amyg_4mm.nii'\n",
    "\n",
    "ROIs = [Lamyg, Ramyg]\n",
    "rois = ['L_amyg','R_amyg']\n",
    "\n",
    "min_clust_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## File handling\n",
    "# Identity node- select subjects\n",
    "infosource = Node(IdentityInterface(fields=['subject_id','ROIs']),\n",
    "                     name='infosource')\n",
    "infosource.iterables = [('subject_id', subjects_list),('ROIs',ROIs)]\n",
    "\n",
    "\n",
    "# Data grabber- select fMRI and ROIs\n",
    "templates = {'func': preproc_dir + '/preproc_func/{subject_id}/func_filtered_smooth.nii',\n",
    "             'motion': output_dir + '/FD_out_metric_values/{subject_id}/FD.txt'}\n",
    "selectfiles = Node(SelectFiles(templates), name='selectfiles')\n",
    "\n",
    "# Datasink- where our select outputs will go\n",
    "datasink = Node(DataSink(), name='datasink')\n",
    "datasink.inputs.base_directory = output_dir\n",
    "datasink.inputs.container = output_dir\n",
    "substitutions = [('_subject_id_', ''),\n",
    "                ('_ROIs_..Users..catcamacho..Box..SNAP..BABIES..ROIs..','')]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Seed-based level 1\n",
    "\n",
    "# Extract ROI timeseries\n",
    "ROI_timeseries = Node(ImageMeants(), name='ROI_timeseries', iterfield='mask')\n",
    "\n",
    "# model ROI connectivity\n",
    "glm = Node(GLM(out_file='betas.nii',out_cope='cope.nii'), name='glm', iterfield='design')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180315-18:16:47,811 workflow INFO:\n",
      "\t Generated workflow graph: /Volumes/iang/active/BABIES/BABIES_rest/workflows/sbc_workflow/graph.dot.png (graph2use=flat, simple_form=True).\n",
      "180315-18:16:47,861 workflow INFO:\n",
      "\t Workflow sbc_workflow settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "180315-18:16:48,30 workflow INFO:\n",
      "\t Running in parallel.\n",
      "180315-18:16:48,35 workflow INFO:\n",
      "\t Currently running 0 tasks, and 2 jobs ready. Free memory (GB): 28.80/28.80, Free processors: 2/2\n",
      "180315-18:16:48,44 workflow INFO:\n",
      "\t Executing node sbc_workflow.selectfiles in dir: /Volumes/iang/active/BABIES/BABIES_rest/workflows/sbc_workflow/_ROIs_..Volumes..iang..active..BABIES..BABIES_rest..ROIs..L_amyg_4mm.nii_subject_id_021-BABIES-T1/selectfiles180315-18:16:48,42 workflow INFO:\n",
      "\t Executing node sbc_workflow.selectfiles in dir: /Volumes/iang/active/BABIES/BABIES_rest/workflows/sbc_workflow/_ROIs_..Volumes..iang..active..BABIES..BABIES_rest..ROIs..R_amyg_4mm.nii_subject_id_021-BABIES-T1/selectfiles\n",
      "\n",
      "180315-18:16:48,208 workflow INFO:\n",
      "\t Running node \"selectfiles\" (\"nipype.interfaces.io.SelectFiles\").180315-18:16:48,230 workflow INFO:\n",
      "\t Running node \"selectfiles\" (\"nipype.interfaces.io.SelectFiles\").\n",
      "\n",
      "180315-18:16:50,47 workflow ERROR:\n",
      "\t Node selectfiles.a1 failed to run on host myelin.stanford.edu.\n",
      "180315-18:16:50,48 workflow ERROR:\n",
      "\t Saving crash info to /Volumes/iang/active/BABIES/BABIES_rest/infant_rest/crash-20180315-181650-myelin-selectfiles.a1-b8359ce0-0c80-4b79-be20-8c7aff55afb4.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1090, in run\n",
      "    outputs = self.aggregate_outputs(runtime)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1155, in aggregate_outputs\n",
      "    predicted_outputs = self._list_outputs()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/io.py\", line 1323, in _list_outputs\n",
      "    raise IOError(msg)\n",
      "OSError: No files were found matching func template: /Volumes/iang/active/BABIES/BABIES_rest/processed/preproc/preproc_func/021-BABIES-T1/func_filtered_smooth.nii\n",
      "\n",
      "180315-18:16:50,63 workflow ERROR:\n",
      "\t Node selectfiles.a0 failed to run on host myelin.stanford.edu.\n",
      "180315-18:16:50,66 workflow ERROR:\n",
      "\t Saving crash info to /Volumes/iang/active/BABIES/BABIES_rest/infant_rest/crash-20180315-181650-myelin-selectfiles.a0-2a328e9f-166d-4396-8407-50f49f7eaa7f.pklz\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/multiproc.py\", line 51, in run_node\n",
      "    result['result'] = node.run(updatehash=updatehash)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 407, in run\n",
      "    self._run_interface()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 517, in _run_interface\n",
      "    self._result = self._run_command(execute)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/nodes.py\", line 650, in _run_command\n",
      "    result = self._interface.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1090, in run\n",
      "    outputs = self.aggregate_outputs(runtime)\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/base.py\", line 1155, in aggregate_outputs\n",
      "    predicted_outputs = self._list_outputs()\n",
      "  File \"/usr/local/anaconda3/lib/python3.6/site-packages/nipype/interfaces/io.py\", line 1323, in _list_outputs\n",
      "    raise IOError(msg)\n",
      "OSError: No files were found matching func template: /Volumes/iang/active/BABIES/BABIES_rest/processed/preproc/preproc_func/021-BABIES-T1/func_filtered_smooth.nii\n",
      "\n",
      "180315-18:16:50,87 workflow INFO:\n",
      "\t Currently running 0 tasks, and 0 jobs ready. Free memory (GB): 28.80/28.80, Free processors: 2/2\n",
      "180315-18:16:52,92 workflow INFO:\n",
      "\t ***********************************\n",
      "180315-18:16:52,93 workflow ERROR:\n",
      "\t could not run node: sbc_workflow.selectfiles.a1\n",
      "180315-18:16:52,94 workflow INFO:\n",
      "\t crashfile: /Volumes/iang/active/BABIES/BABIES_rest/infant_rest/crash-20180315-181650-myelin-selectfiles.a1-b8359ce0-0c80-4b79-be20-8c7aff55afb4.pklz\n",
      "180315-18:16:52,95 workflow ERROR:\n",
      "\t could not run node: sbc_workflow.selectfiles.a0\n",
      "180315-18:16:52,97 workflow INFO:\n",
      "\t crashfile: /Volumes/iang/active/BABIES/BABIES_rest/infant_rest/crash-20180315-181650-myelin-selectfiles.a0-2a328e9f-166d-4396-8407-50f49f7eaa7f.pklz\n",
      "180315-18:16:52,98 workflow INFO:\n",
      "\t ***********************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Workflow did not execute cleanly. Check log for details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-da3334993771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msbc_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkflow_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msbc_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph2use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msbc_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MultiProc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_procs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mproc_cores\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/engine/workflows.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, plugin, plugin_args, updatehash)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_report_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdatehash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdatehash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%dT%H%M%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr2bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'execution'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'write_provenance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, graph, config, updatehash)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_node_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mreport_nodes_not_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# close any open resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/nipype/pipeline/plugins/tools.py\u001b[0m in \u001b[0;36mreport_nodes_not_run\u001b[0;34m(notrun)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***********************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         raise RuntimeError(('Workflow did not execute cleanly. '\n\u001b[0m\u001b[1;32m     80\u001b[0m                             'Check log for details'))\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Workflow did not execute cleanly. Check log for details"
     ]
    }
   ],
   "source": [
    "sbc_workflow = Workflow(name='sbc_workflow')\n",
    "sbc_workflow.connect([(infosource,selectfiles,[('subject_id','subject_id')]),\n",
    "                      (selectfiles,ROI_timeseries,[('func','in_file')]),\n",
    "                      (infosource,ROI_timeseries,[('ROIs','mask')]),\n",
    "                      (ROI_timeseries,glm,[('out_file','design')]),\n",
    "                      (selectfiles,glm,[('func','in_file')]),\n",
    "                      (ROI_timeseries, datasink, [('out_file','roi_ts')]),\n",
    "                      (glm,datasink,[('out_cope','glm_seed_copes')]),\n",
    "                      (glm,datasink,[('out_file','glm_betas')])\n",
    "                     ])\n",
    "sbc_workflow.base_dir = workflow_dir\n",
    "sbc_workflow.write_graph(graph2use='flat')\n",
    "sbc_workflow.run('MultiProc', plugin_args={'n_procs': proc_cores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infosource2 = Node(IdentityInterface(fields=['roi']),\n",
    "                   name='infosource2')\n",
    "infosource2.iterables = ('roi',rois)\n",
    "\n",
    "\n",
    "# Data grabber- select fMRI and ROIs\n",
    "templates = {'roi': 'glm_betas/%s_4mm.nii*/betas.nii'}\n",
    "\n",
    "datagrabber = Node(DataGrabber(infields=['roi'], \n",
    "                               outfields=['roi'],\n",
    "                               sort_filelist=True,\n",
    "                               base_directory=output_dir,\n",
    "                               template='glm_betas/%s_4mm.nii*/betas.nii',\n",
    "                               field_template=templates,\n",
    "                               template_args=dict(roi=[['roi']])),\n",
    "                   name='datagrabber')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_cluster_betas(cluster_index_file, sample_betas, min_clust_size, subject_ids):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from pandas import DataFrame, Series\n",
    "    from numpy import unique, zeros_like\n",
    "    from nipype.interfaces.fsl.utils import ImageMeants\n",
    "    from os.path import abspath\n",
    "    \n",
    "    subject_ids = sorted(subject_ids)\n",
    "    sample_data = DataFrame(subject_ids, index=None, columns=['Subject'])\n",
    "    \n",
    "    cluster_nifti = load(cluster_index_file[0])\n",
    "    cluster_data = cluster_nifti.get_data()\n",
    "    clusters, cluster_sizes = unique(cluster_data, return_counts=True)\n",
    "    \n",
    "    final_clusters = clusters[cluster_sizes>=min_cluster_size]\n",
    "    for clust_num in final_clusters[1:]:\n",
    "        temp = zeros_like(cluster_data)\n",
    "        temp[cluster_data==clust_num] = 1\n",
    "        temp_nii = Nifti1Image(temp,cluster_nifti.affine)\n",
    "        temp_file = 'temp_clust_mask.nii'\n",
    "        save(temp_nii, temp_file)\n",
    "        \n",
    "        eb = ImageMeants()\n",
    "        eb.inputs.in_file = sample_betas\n",
    "        eb.inputs.mask = temp_file\n",
    "        eb.inputs.out_file = 'betas.txt'\n",
    "        eb.run()\n",
    "        betas = open('betas.txt').read().splitlines()\n",
    "        sample_data['clust' + str(clust_num)] = Series(betas, index=sample_data.index)\n",
    "    \n",
    "    sample_data.to_csv('extracted_betas.csv')\n",
    "    extracted_betas_csv = abspath('extracted_betas.csv')\n",
    "    return(extracted_betas_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Level 2\n",
    "\n",
    "# merge param estimates across all subjects per seed\n",
    "merge = Node(Merge(dimension='t'),\n",
    "             name='merge')\n",
    "\n",
    "# FSL randomise for higher level analysis\n",
    "highermodel = Node(Randomise(tfce=True,\n",
    "                             raw_stats_imgs= True,\n",
    "                             design_mat=group_mat,\n",
    "                             mask=gm_template,\n",
    "                             tcon=group_con),\n",
    "                   name = 'highermodel')\n",
    "\n",
    "## Cluster results\n",
    "# make binary masks of sig clusters\n",
    "binarize = MapNode(Binarize(min=0.95, max=1.0), \n",
    "                   name='binarize', \n",
    "                   iterfield=['in_file'])\n",
    "\n",
    "# mask T-map before clustering\n",
    "mask_tmaps = MapNode(ApplyMask(), \n",
    "                     name='mask_tmaps',\n",
    "                     iterfield=['in_file','mask_file'])\n",
    "\n",
    "# clusterize and extract cluster stats/peaks\n",
    "clusterize = MapNode(Cluster(threshold=3.5, \n",
    "                             out_index_file='outindex.nii', \n",
    "                             out_localmax_txt_file='localmax.txt'), \n",
    "                     name='clusterize',\n",
    "                     iterfield=['in_file'])\n",
    "\n",
    "extract_betas = Node(Function(input_names=['cluster_index_file','sample_betas','min_clust_size','subject_ids'],\n",
    "                              output_names=['extracted_betas_csv'],\n",
    "                              function=extract_cluster_betas),\n",
    "                     name='extract_betas')\n",
    "extract_betas.inputs.min_clust_size = min_clust_size\n",
    "extract_betas.inputs.subject_ids = subjects_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group_workflow = Workflow(name='group_workflow')\n",
    "group_workflow.connect([(infosource2,datagrabber,[('roi','roi')]),\n",
    "                        (datagrabber,merge,[('roi','in_files')]),\n",
    "                        (merge,highermodel,[('merged_file','in_file')]),\n",
    "                        (highermodel, binarize, [('t_corrected_p_files','in_file')]),\n",
    "                        (binarize, mask_tmaps, [('binary_file','mask_file')]),\n",
    "                        (highermodel, mask_tmaps, [('tstat_files','in_file')]),\n",
    "                        (mask_tmaps, clusterize, [('out_file','in_file')]),\n",
    "                        #(merge, extract_betas, [('merged_file','sample_betas')]),\n",
    "                        #(clusterize, extract_betas, [('index_file','cluster_index_file')]),\n",
    "\n",
    "                        (highermodel,datasink,[('t_corrected_p_files','rand_corrp_files')]),\n",
    "                        (highermodel,datasink,[('tstat_files','rand_tstat_files')]),\n",
    "                        (clusterize,datasink,[('index_file','cluster_index_file')]),\n",
    "                        (clusterize,datasink,[('localmax_txt_file','localmax_txt_file')]),\n",
    "                        (merge, datasink, [('merged_file','merged_betas')])\n",
    "                        #(extract_betas, datasink, [('extracted_betas_file','all_cluster_betas')])\n",
    "                       ])\n",
    "group_workflow.base_dir = workflow_dir\n",
    "group_workflow.write_graph(graph2use='flat')\n",
    "group_workflow.run('MultiProc', plugin_args={'n_procs': proc_cores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
