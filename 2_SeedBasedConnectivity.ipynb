{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "from nipype.interfaces.io import DataSink, SelectFiles, DataGrabber # Data i/o\n",
    "from nipype.interfaces.utility import IdentityInterface, Function     # utility\n",
    "from nipype.pipeline.engine import Node, Workflow, JoinNode,MapNode      # pypeline engine\n",
    "\n",
    "from nipype.interfaces.fsl.model import Randomise, GLM, Cluster\n",
    "from nipype.interfaces.freesurfer.model import Binarize\n",
    "from nipype.interfaces.fsl.utils import ImageMeants, Merge, Split\n",
    "from nipype.interfaces.fsl.maths import ApplyMask\n",
    "from pandas import read_csv, DataFrame\n",
    "\n",
    "#set output file type for FSL to NIFTI\n",
    "from nipype.interfaces.fsl.preprocess import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI_GZ')\n",
    "\n",
    "# Set study variables\n",
    "setup = \"Lucy\"\n",
    "\n",
    "if setup == \"sherlock\":\n",
    "    studyhome = '/oak/stanford/groups/iang/BABIES_data/BABIES_rest/'\n",
    "    preproc_dir = studyhome + 'processed/preproc'\n",
    "    output_dir = studyhome + 'processed/sbc_analysis'\n",
    "    workflow_dir = studyhome + 'workflows'\n",
    "    roi_dir = studyhome + 'ROIs'\n",
    "    group_con = studyhome + 'misc/tcon.con'\n",
    "    template_brain = studyhome + 'templates/T2w_BABIES_template_2mm.nii.gz'\n",
    "    gm_template = studyhome + 'templates/BABIES_gm_mask_2mm.nii.gz'\n",
    "\n",
    "\n",
    "if setup == \"Lucy\":\n",
    "    studyhome = '/Volumes/iang/BABIES_data/BABIES_rest/'\n",
    "    preproc_dir = studyhome + 'processed/preproc'\n",
    "    output_dir = studyhome + 'processed/sbc_analysis'\n",
    "    workflow_dir = studyhome + 'workflows'\n",
    "    roi_dir = studyhome + 'ROIs'\n",
    "    group_con = studyhome + 'misc/tcon.con'\n",
    "    template_brain = studyhome + 'templates/T2w_BABIES_template_2mm.nii.gz'\n",
    "    gm_template = studyhome + 'templates/BABIES_gm_mask_2mm.nii.gz'\n",
    "    \n",
    "\n",
    "elif setup == \"Cat\":\n",
    "    studyhome = '/Users/catcamacho/Box/SNAP/BABIES/BABIES_rest/'\n",
    "    preproc_dir = studyhome + 'processed/preproc'\n",
    "    output_dir = studyhome + 'processed/sbc_analysis'\n",
    "    workflow_dir = studyhome + 'workflows'\n",
    "    roi_dir = studyhome + 'ROIs'\n",
    "    group_con = studyhome + 'misc/tcon.con'\n",
    "    template_brain = studyhome + 'templates/T2w_BABIES_template_2mm.nii.gz'\n",
    "    gm_template = studyhome + 'templates/BABIES_gm_mask_2mm.nii.gz'\n",
    "\n",
    "subject_info = read_csv(studyhome + 'misc/subject_info.csv', index_col=None) #csv with columns 'subject_id','age', and any group lists like 'lena'\n",
    "subject_info = subject_info.sort_values(by=['subject_id'])\n",
    "subjects_list = subject_info['subject_id'].tolist()\n",
    "list_names = ['lena','sens', 'postnatal_stress'] #replace with actual group list labels\n",
    "\n",
    "proc_cores = 2\n",
    "\n",
    "# ROIs for connectivity analysis\n",
    "Lamyg = roi_dir + '/L_amyg_6mm.nii.gz'\n",
    "Ramyg = roi_dir + '/R_amyg_6mm.nii.gz'\n",
    "lipl = roi_dir + '/l_ipl_8mm.nii.gz'\n",
    "ripl = roi_dir + '/r_ipl_8mm.nii.gz'\n",
    "mpfc = roi_dir + '/mPFC_8mm.nii.gz'\n",
    "\n",
    "target_rois = [mpfc, lipl, ripl]\n",
    "\n",
    "ROIs = [Lamyg, Ramyg]\n",
    "rois = ['L_amyg','R_amyg']\n",
    "\n",
    "#Lifg = roi_dir + '/l_ifg_10mm.nii.gz'\n",
    "#Rifg = roi_dir + '/r_ifg_10mm.nii.gz'\n",
    "\n",
    "#ROIs = [Lifg, Rifg]\n",
    "#rois = ['l_ifg','r_ifg']\n",
    "\n",
    "min_clust_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## File handling\n",
    "# Identity node- select subjects\n",
    "infosource = Node(IdentityInterface(fields=['subject_id','ROIs']),\n",
    "                     name='infosource')\n",
    "infosource.iterables = [('subject_id', subjects_list),('ROIs',ROIs)]\n",
    "\n",
    "\n",
    "# Data grabber- select fMRI and ROIs\n",
    "templates = {'func': preproc_dir + '/proc_func/{subject_id}/func_filtered_smooth.nii*'}\n",
    "selectfiles = Node(SelectFiles(templates), name='selectfiles')\n",
    "\n",
    "# Datasink- where our select outputs will go\n",
    "datasink = Node(DataSink(), name='datasink')\n",
    "datasink.inputs.base_directory = output_dir\n",
    "datasink.inputs.container = output_dir\n",
    "substitutions = [('_subject_id_', ''),\n",
    "                ('_ROIs_..Users..catcamacho..Box..SNAP..BABIES..BABIES_rest..ROIs..',''), \n",
    "                 ('_ROIs_..share..iang..active..BABIES..BABIES_rest..ROIs..',''), \n",
    "                  ('_ROIs_..oak..stanford..groups..iang..BABIES_data..BABIES_rest..ROIs..',''),\n",
    "                 ('_ROIs_..Volumes..iang..BABIES_data..BABIES_rest..ROIs..',''),\n",
    "                 ('_6mm.nii.gz','_'),('_8mm.nii.gz','_'),('_10mm.nii.gz','_')]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Seed-based level 1\n",
    "\n",
    "# Extract ROI timeseries\n",
    "ROI_timeseries = Node(ImageMeants(), name='ROI_timeseries', iterfield='mask')\n",
    "\n",
    "# model ROI connectivity\n",
    "glm = Node(GLM(out_file='betas.nii.gz',out_cope='cope.nii.gz'), name='glm', iterfield='design')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sbc_workflow = Workflow(name='sbc_workflow')\n",
    "sbc_workflow.connect([(infosource,selectfiles,[('subject_id','subject_id')]),\n",
    "                      (selectfiles,ROI_timeseries,[('func','in_file')]),\n",
    "                      (infosource,ROI_timeseries,[('ROIs','mask')]),\n",
    "                      (ROI_timeseries,glm,[('out_file','design')]),\n",
    "                      (selectfiles,glm,[('func','in_file')]),\n",
    "                      (glm,datasink,[('out_file','glm_betas')])\n",
    "                     ])\n",
    "sbc_workflow.base_dir = workflow_dir\n",
    "#sbc_workflow.write_graph(graph2use='flat')\n",
    "sbc_workflow.run('MultiProc', plugin_args={'n_procs': proc_cores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_cluster_betas(cluster_index_file, sample_betas, min_clust_size, subject_ids):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from pandas import DataFrame, Series\n",
    "    from numpy import unique, zeros_like\n",
    "    from nipype.interfaces.fsl.utils import ImageMeants\n",
    "    from os.path import abspath\n",
    "    \n",
    "    subject_ids = sorted(subject_ids)\n",
    "    sample_data = DataFrame(subject_ids, index=None, columns=['Subject'])\n",
    "    \n",
    "    cluster_nifti = load(cluster_index_file[0])\n",
    "    cluster_data = cluster_nifti.get_data()\n",
    "    clusters, cluster_sizes = unique(cluster_data, return_counts=True)\n",
    "    \n",
    "    final_clusters = clusters[cluster_sizes>=min_clust_size]\n",
    "    for clust_num in final_clusters[1:]:\n",
    "        temp = zeros_like(cluster_data)\n",
    "        temp[cluster_data==clust_num] = 1\n",
    "        temp_nii = Nifti1Image(temp,cluster_nifti.affine)\n",
    "        temp_file = 'temp_clust_mask.nii.gz'\n",
    "        save(temp_nii, temp_file)\n",
    "        \n",
    "        eb = ImageMeants()\n",
    "        eb.inputs.in_file = sample_betas\n",
    "        eb.inputs.mask = temp_file\n",
    "        eb.inputs.out_file = 'betas.txt'\n",
    "        eb.run()\n",
    "        betas = open('betas.txt').read().splitlines()\n",
    "        sample_data['clust' + str(clust_num)] = Series(betas, index=sample_data.index)\n",
    "    \n",
    "    sample_data.to_csv('extracted_betas.csv')\n",
    "    extracted_betas_csv = abspath('extracted_betas.csv')\n",
    "    return(extracted_betas_csv)\n",
    "\n",
    "def separate_sub_lists(subjects_df, list_name, output_dir, roi):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from subprocess import check_call\n",
    "    from os.path import abspath\n",
    "    from nipype.interfaces.fsl import Merge\n",
    "    \n",
    "    included_subs = subjects_df[subjects_df[list_name]==1]\n",
    "    included_subs = included_subs.sort_values(by=['subject_id'])\n",
    "    subjects_list = included_subs['subject_id'].tolist()\n",
    "    subjects_ages = included_subs['age'].tolist()\n",
    "    \n",
    "    beta_list = []\n",
    "    text_file = open('temp_text.txt','w')\n",
    "    for a in range(0,len(subjects_list)):\n",
    "        beta_list.append(output_dir+'/glm_betas/'+roi+'_'+subjects_list[a]+'/betas.nii.gz')\n",
    "        text_file.write('1 {0}\\n'.format(subjects_ages[a]))\n",
    "    \n",
    "    text_file.close()\n",
    "    file = abspath('temp_text.txt')\n",
    "    check_call(['Text2Vest',file,list_name + '_design.mat'])\n",
    "    design_file = abspath('{0}_design.mat'.format(list_name))\n",
    "        \n",
    "    me=Merge()\n",
    "    me.inputs.dimension='t'\n",
    "    me.inputs.in_files=beta_list\n",
    "    me.inputs.merged_file='betas_merged.nii.gz'\n",
    "    me.run()\n",
    "    \n",
    "    betas = abspath('betas_merged.nii.gz')\n",
    "    \n",
    "    return(betas,design_file,subjects_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Level 2\n",
    "# new identity node\n",
    "infosource2 = Node(IdentityInterface(fields=['roi','list_name']),\n",
    "                   name='infosource2')\n",
    "infosource2.iterables = [('roi',rois),('list_name',list_names)]\n",
    "\n",
    "# Data grabber- select fMRI and ROIs\n",
    "datagrabber = Node(Function(input_names=['subjects_df', 'list_name', 'output_dir', 'roi'], \n",
    "                            output_names=['betas','design_file', 'subjects_list'], \n",
    "                            function=separate_sub_lists), name='datagrabber')\n",
    "datagrabber.inputs.output_dir = output_dir\n",
    "datagrabber.inputs.subjects_df = subject_info\n",
    "\n",
    "# FSL randomise for higher level analysis\n",
    "highermodel = Node(Randomise(raw_stats_imgs= True,\n",
    "                             mask=gm_template,\n",
    "                             tcon=group_con),\n",
    "                   name = 'highermodel')\n",
    "\n",
    "## Cluster results\n",
    "# make binary masks of sig clusters\n",
    "binarize = MapNode(Binarize(min=0.95), \n",
    "                   name='binarize', \n",
    "                   iterfield=['in_file'])\n",
    "\n",
    "# mask T-map before clustering\n",
    "mask_tmaps = MapNode(ApplyMask(), \n",
    "                     name='mask_tmaps',\n",
    "                     iterfield=['in_file','mask_file'])\n",
    "\n",
    "# clusterize and extract cluster stats/peaks\n",
    "clusterize = MapNode(Cluster(threshold=3.56, #t-stat for p=.001 (calculate based on N); default is FWE\n",
    "                             out_index_file='outindex.nii.gz', \n",
    "                             out_localmax_txt_file='localmax.txt'), \n",
    "                     name='clusterize',\n",
    "                     iterfield=['in_file'])\n",
    "\n",
    "extract_betas = Node(Function(input_names=['cluster_index_file','sample_betas',\n",
    "                                           'min_clust_size','subject_ids'],\n",
    "                              output_names=['extracted_betas_csv'],\n",
    "                              function=extract_cluster_betas),\n",
    "                     name='extract_betas')\n",
    "extract_betas.inputs.min_clust_size = min_clust_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group_workflow = Workflow(name='group_workflow')\n",
    "group_workflow.connect([(infosource2,datagrabber,[('roi','roi')]),\n",
    "                        (infosource2,datagrabber,[('list_name','list_name')]),\n",
    "                        (datagrabber, highermodel,[('betas','in_file'),('design_file','design_mat')]),\n",
    "                        (highermodel, clusterize, [('tstat_files','in_file')]),\n",
    "                        (datagrabber, extract_betas, [('betas','sample_betas'),\n",
    "                                                      ('subjects_list','subject_ids')]),\n",
    "                        (clusterize, extract_betas, [('index_file','cluster_index_file')]),\n",
    "\n",
    "                        (highermodel,datasink,[('t_corrected_p_files','rand_corrp_files')]),\n",
    "                        (highermodel,datasink,[('tstat_files','rand_tstat_files')]),\n",
    "                        (clusterize,datasink,[('index_file','cluster_index_file')]),\n",
    "                        (clusterize,datasink,[('localmax_txt_file','localmax_txt_file')]),\n",
    "                        (datagrabber, datasink, [('betas','merged_betas')]),\n",
    "                        (extract_betas, datasink, [('extracted_betas_csv','all_cluster_betas')])\n",
    "                       ])\n",
    "group_workflow.base_dir = workflow_dir\n",
    "#group_workflow.write_graph(graph2use='flat')\n",
    "group_workflow.run('MultiProc', plugin_args={'n_procs': proc_cores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def target_roi_betas(target_rois, sample_betas, subject_ids):\n",
    "    from nipype.interfaces.fsl.utils import ImageMeants\n",
    "    from os.path import abspath, basename\n",
    "    from pandas import DataFrame, Series\n",
    "    from nipype import config, logging\n",
    "\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "\n",
    "    subject_ids = sorted(subject_ids)\n",
    "    sample_data = DataFrame(subject_ids, index=None, columns=['Subject'])\n",
    "\n",
    "    for roi in target_rois: \n",
    "        roi_file = basename(roi)\n",
    "        eb = ImageMeants()\n",
    "        eb.inputs.in_file = sample_betas\n",
    "        eb.inputs.mask = roi\n",
    "        eb.inputs.out_file = 'betas.txt'\n",
    "        eb.run()\n",
    "        betas = open('betas.txt').read().splitlines()\n",
    "        sample_data[roi_file[:-7]] = Series(betas, index=sample_data.index)\n",
    "\n",
    "    sample_data.to_csv('extracted_betas.csv')\n",
    "    extracted_betas_csv = abspath('extracted_betas.csv')\n",
    "    return(extracted_betas_csv)\n",
    "\n",
    "target_betas = Node(Function(input_names=['target_rois','sample_betas','subject_ids'], \n",
    "                             output_names=['extracted_betas_csv'], \n",
    "                             function=target_roi_betas),\n",
    "                    name='target_betas')\n",
    "target_betas.inputs.target_rois = target_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190122-12:54:45,364 workflow INFO:\n",
      "\t Workflow pullbetas_workflow settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "190122-12:54:48,416 workflow INFO:\n",
      "\t Running in parallel.\n",
      "190122-12:54:48,428 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 6 jobs ready. Free memory (GB): 14.40/14.40, Free processors: 2/2.\n",
      "190122-12:54:49,579 workflow INFO:\n",
      "\t [Job 0] Cached (pullbetas_workflow.datagrabber).\n",
      "190122-12:54:50,15 workflow INFO:\n",
      "\t [Job 3] Cached (pullbetas_workflow.datagrabber).\n",
      "190122-12:54:51,12 workflow INFO:\n",
      "\t [Job 1] Cached (pullbetas_workflow.target_betas).\n",
      "190122-12:54:51,392 workflow INFO:\n",
      "\t [Job 4] Cached (pullbetas_workflow.target_betas).\n",
      "190122-12:54:53,964 workflow INFO:\n",
      "\t [Node] Setting-up \"pullbetas_workflow.datasink\" in \"/Volumes/iang/BABIES_data/BABIES_rest/workflows/pullbetas_workflow/_list_name_postnatal_stress_roi_R_amyg/datasink\".\n",
      "190122-12:54:54,740 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 4 jobs ready. Free memory (GB): 14.00/14.40, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * pullbetas_workflow.datasink\n",
      "                       * pullbetas_workflow.datasink\n",
      "190122-12:54:54,745 workflow INFO:\n",
      "\t [Node] Setting-up \"pullbetas_workflow.datasink\" in \"/Volumes/iang/BABIES_data/BABIES_rest/workflows/pullbetas_workflow/_list_name_sens_roi_R_amyg/datasink\".\n",
      "190122-12:54:57,735 workflow INFO:\n",
      "\t [Node] Running \"datasink\" (\"nipype.interfaces.io.DataSink\")\n",
      "190122-12:55:00,247 workflow INFO:\n",
      "\t [Node] Running \"datasink\" (\"nipype.interfaces.io.DataSink\")\n",
      "190122-12:55:03,265 workflow INFO:\n",
      "\t [Node] Finished \"pullbetas_workflow.datasink\".190122-12:55:03,179 workflow INFO:\n",
      "\t [Node] Finished \"pullbetas_workflow.datasink\".\n",
      "\n",
      "190122-12:55:04,755 workflow INFO:\n",
      "\t [Job 2] Completed (pullbetas_workflow.datasink).\n",
      "190122-12:55:04,759 workflow INFO:\n",
      "\t [Job 5] Completed (pullbetas_workflow.datasink).\n",
      "190122-12:55:04,764 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 4 jobs ready. Free memory (GB): 14.40/14.40, Free processors: 2/2.\n",
      "190122-12:55:06,849 workflow INFO:\n",
      "\t [Job 6] Cached (pullbetas_workflow.datagrabber).\n",
      "190122-12:55:07,3 workflow INFO:\n",
      "\t [Job 9] Cached (pullbetas_workflow.datagrabber).\n",
      "190122-12:55:07,413 workflow INFO:\n",
      "\t [Job 7] Cached (pullbetas_workflow.target_betas).\n",
      "190122-12:55:07,677 workflow INFO:\n",
      "\t [Job 10] Cached (pullbetas_workflow.target_betas).\n",
      "190122-12:55:09,969 workflow INFO:\n",
      "\t [Node] Setting-up \"pullbetas_workflow.datasink\" in \"/Volumes/iang/BABIES_data/BABIES_rest/workflows/pullbetas_workflow/_list_name_lena_roi_R_amyg/datasink\".\n",
      "190122-12:55:10,255 workflow INFO:\n",
      "\t [Node] Setting-up \"pullbetas_workflow.datasink\" in \"/Volumes/iang/BABIES_data/BABIES_rest/workflows/pullbetas_workflow/_list_name_postnatal_stress_roi_L_amyg/datasink\".\n",
      "190122-12:55:11,13 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 2 jobs ready. Free memory (GB): 14.00/14.40, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * pullbetas_workflow.datasink\n",
      "                       * pullbetas_workflow.datasink\n",
      "190122-12:55:15,402 workflow INFO:\n",
      "\t [Node] Running \"datasink\" (\"nipype.interfaces.io.DataSink\")190122-12:55:15,421 workflow INFO:\n",
      "\t [Node] Running \"datasink\" (\"nipype.interfaces.io.DataSink\")\n",
      "\n",
      "190122-12:55:18,188 workflow INFO:\n",
      "\t [Node] Finished \"pullbetas_workflow.datasink\".\n",
      "190122-12:55:18,704 workflow INFO:\n",
      "\t [Node] Finished \"pullbetas_workflow.datasink\".\n",
      "190122-12:55:19,29 workflow INFO:\n",
      "\t [Job 8] Completed (pullbetas_workflow.datasink).\n",
      "190122-12:55:19,31 workflow INFO:\n",
      "\t [Job 11] Completed (pullbetas_workflow.datasink).\n",
      "190122-12:55:19,35 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 2 jobs ready. Free memory (GB): 14.40/14.40, Free processors: 2/2.\n",
      "190122-12:55:19,308 workflow INFO:\n",
      "\t [Job 12] Cached (pullbetas_workflow.datagrabber).\n",
      "190122-12:55:19,419 workflow INFO:\n",
      "\t [Job 15] Cached (pullbetas_workflow.datagrabber).\n",
      "190122-12:55:21,370 workflow INFO:\n",
      "\t [Job 13] Cached (pullbetas_workflow.target_betas).\n",
      "190122-12:55:21,657 workflow INFO:\n",
      "\t [Job 16] Cached (pullbetas_workflow.target_betas).\n",
      "190122-12:55:23,988 workflow INFO:\n",
      "\t [Node] Setting-up \"pullbetas_workflow.datasink\" in \"/Volumes/iang/BABIES_data/BABIES_rest/workflows/pullbetas_workflow/_list_name_sens_roi_L_amyg/datasink\".\n",
      "190122-12:55:24,552 workflow INFO:\n",
      "\t [Node] Setting-up \"pullbetas_workflow.datasink\" in \"/Volumes/iang/BABIES_data/BABIES_rest/workflows/pullbetas_workflow/_list_name_lena_roi_L_amyg/datasink\".\n",
      "190122-12:55:25,37 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 0 jobs ready. Free memory (GB): 14.00/14.40, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * pullbetas_workflow.datasink\n",
      "                       * pullbetas_workflow.datasink\n",
      "190122-12:55:27,706 workflow INFO:\n",
      "\t [Node] Running \"datasink\" (\"nipype.interfaces.io.DataSink\")\n",
      "190122-12:55:28,309 workflow INFO:\n",
      "\t [Node] Running \"datasink\" (\"nipype.interfaces.io.DataSink\")\n",
      "190122-12:55:30,172 workflow INFO:\n",
      "\t [Node] Finished \"pullbetas_workflow.datasink\".190122-12:55:30,313 workflow INFO:\n",
      "\t [Node] Finished \"pullbetas_workflow.datasink\".\n",
      "\n",
      "190122-12:55:31,43 workflow INFO:\n",
      "\t [Job 14] Completed (pullbetas_workflow.datasink).\n",
      "190122-12:55:31,45 workflow INFO:\n",
      "\t [Job 17] Completed (pullbetas_workflow.datasink).\n",
      "190122-12:55:31,48 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 0 jobs ready. Free memory (GB): 14.40/14.40, Free processors: 2/2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x11d92eef0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pullbetas_workflow = Workflow(name='pullbetas_workflow')\n",
    "pullbetas_workflow.connect([(infosource2,datagrabber,[('roi','roi'),\n",
    "                                                      ('list_name','list_name')]),\n",
    "                            (datagrabber, target_betas,[('betas','sample_betas'),\n",
    "                                                        ('subjects_list','subject_ids')]),\n",
    "                            (target_betas, datasink, [('extracted_betas_csv','roi_target_betas')])\n",
    "                           ])\n",
    "pullbetas_workflow.base_dir = workflow_dir\n",
    "#pullbetas_workflow.write_graph(graph2use='flat')\n",
    "pullbetas_workflow.run('MultiProc', plugin_args={'n_procs': proc_cores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
